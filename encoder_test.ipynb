{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Simple Testing of LLaMA encoder\n",
    "- We will be using the following tech stack\n",
    "\n",
    "   - HuggingFace Transformers\n",
    " \n",
    "   - Modal GPU credits for local file gpu usage\n",
    " \n",
    "  - EmojiLM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dda2895e6a04a4f9d64994cbff82040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import huggingface_hub\n",
    "import os\n",
    "load_dotenv()\n",
    "huggingface_hub.notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31971/90382873.py:24: DeprecationError: 2025-02-03: Modal will stop implicitly adding local Python modules to the Image (\"automounting\") in a future update. The following modules need to be explicitly added for future compatibility:\n",
      "* _remote_module_non_scriptable\n",
      "\n",
      "e.g.:\n",
      "image_with_source = my_image.add_local_python_source(\"_remote_module_non_scriptable\")\n",
      "\n",
      "For more information, see https://modal.com/docs/guide/modal-1-0-migration\n",
      "  def get_model_function(model_name):\n",
      "/tmp/ipykernel_31971/90382873.py:92: DeprecationError: 2025-02-03: Modal will stop implicitly adding local Python modules to the Image (\"automounting\") in a future update. The following modules need to be explicitly added for future compatibility:\n",
      "* _remote_module_non_scriptable\n",
      "\n",
      "e.g.:\n",
      "image_with_source = my_image.add_local_python_source(\"_remote_module_non_scriptable\")\n",
      "\n",
      "For more information, see https://modal.com/docs/guide/modal-1-0-migration\n",
      "  def get_embeddings(text, model_name):\n",
      "/tmp/ipykernel_31971/90382873.py:120: DeprecationError: 2025-02-03: Modal will stop implicitly adding local Python modules to the Image (\"automounting\") in a future update. The following modules need to be explicitly added for future compatibility:\n",
      "* _remote_module_non_scriptable\n",
      "\n",
      "e.g.:\n",
      "image_with_source = my_image.add_local_python_source(\"_remote_module_non_scriptable\")\n",
      "\n",
      "For more information, see https://modal.com/docs/guide/modal-1-0-migration\n",
      "  def do_inference(text, model_name):\n",
      "/tmp/ipykernel_31971/90382873.py:136: DeprecationError: 2025-02-03: Modal will stop implicitly adding local Python modules to the Image (\"automounting\") in a future update. The following modules need to be explicitly added for future compatibility:\n",
      "* _remote_module_non_scriptable\n",
      "\n",
      "e.g.:\n",
      "image_with_source = my_image.add_local_python_source(\"_remote_module_non_scriptable\")\n",
      "\n",
      "For more information, see https://modal.com/docs/guide/modal-1-0-migration\n",
      "  def get_intermediate_outputs(text, model_name, layer_num=4):\n"
     ]
    }
   ],
   "source": [
    "# Firstly, grab the LLaMA encoder and the dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import modal\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "huggingface_secret = modal.Secret.from_name(\n",
    "    \"huggingface-secret\", required_keys=[\"HF_TOKEN\"]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "volume = modal.Volume.from_name(\n",
    "    \"llama_embeddings\", create_if_missing=True\n",
    ")\n",
    "MODEL_DIR = \"/model\"\n",
    "\n",
    "# Define stubs at module level, outside the class\n",
    "app = modal.App(name=\"llama-embeddings\")\n",
    "image = modal.Image.debian_slim(python_version = '3.10').pip_install([\"transformers\", \"torch\", \"accelerate\", \"hf_transfer\"])\n",
    "image = image.env(\n",
    "    {\"HF_HUB_ENABLE_HF_TRANSFER\": \"1\"}  # turn on faster downloads from HF\n",
    ")\n",
    "\n",
    "@app.function(image=image, gpu=\"any\", secrets=[huggingface_secret], timeout=600, volumes={MODEL_DIR: volume}, memory=16384)\n",
    "def get_model_function(model_name):\n",
    "    from transformers import AutoModelForCausalLM\n",
    "    import os\n",
    "    \n",
    "    print(f\"Starting to load model: {model_name}\")\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    \n",
    "    # Log the model being loaded\n",
    "    print(\"Loading model... (this may take a few minutes)\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=MODEL_DIR, device_map=\"cuda\")\n",
    "    print(f\"Model loaded successfully!\")\n",
    "    \n",
    "    # Print model architecture details\n",
    "    print(f\"Model architecture: {model.__class__.__name__}\")\n",
    "    print(f\"Number of layers: {len(model.model.layers)}\")\n",
    "    \n",
    "    # Create serializable information about some sample layers\n",
    "    layers_info = {\n",
    "        \"num_layers\": len(model.model.layers),\n",
    "        \"hidden_size\": model.config.hidden_size if hasattr(model.config, \"hidden_size\") else \"Unknown\"\n",
    "    }\n",
    "    \n",
    "    # Information for all layers to make it serializable\n",
    "    layers_info[\"all_layers\"] = []\n",
    "    for i in range(len(model.model.layers)):\n",
    "        layer = model.model.layers[i]\n",
    "        layer_info = {\n",
    "            \"index\": i,\n",
    "            \"type\": layer.__class__.__name__,\n",
    "            \"parameter_count\": sum(p.numel() for p in layer.parameters())\n",
    "        }\n",
    "        \n",
    "        # Extract information about modules in the decoder layer\n",
    "        if hasattr(layer, \"self_attn\"):\n",
    "            layer_info[\"modules\"] = {\n",
    "                \"self_attention\": {\n",
    "                    \"type\": layer.self_attn.__class__.__name__,\n",
    "                    \"parameter_count\": sum(p.numel() for p in layer.self_attn.parameters())\n",
    "                },\n",
    "                \"mlp\": {\n",
    "                    \"type\": layer.mlp.__class__.__name__,\n",
    "                    \"parameter_count\": sum(p.numel() for p in layer.mlp.parameters())\n",
    "                }\n",
    "            }\n",
    "            if hasattr(layer, \"input_layernorm\"):\n",
    "                layer_info[\"modules\"][\"input_layernorm\"] = {\n",
    "                    \"type\": layer.input_layernorm.__class__.__name__,\n",
    "                    \"parameter_count\": sum(p.numel() for p in layer.input_layernorm.parameters())\n",
    "                }\n",
    "            if hasattr(layer, \"post_attention_layernorm\"):\n",
    "                layer_info[\"modules\"][\"post_attention_layernorm\"] = {\n",
    "                    \"type\": layer.post_attention_layernorm.__class__.__name__,\n",
    "                    \"parameter_count\": sum(p.numel() for p in layer.post_attention_layernorm.parameters())\n",
    "                }\n",
    "        \n",
    "        layers_info[\"all_layers\"].append(layer_info)\n",
    "        \n",
    "        # Print details for the first few layers\n",
    "        if i < 3:\n",
    "            print(f\"Layer {i}: {layer_info['type']} with {layer_info['parameter_count']:,} parameters\")\n",
    "            if \"modules\" in layer_info:\n",
    "                for module_name, module_info in layer_info[\"modules\"].items():\n",
    "                    print(f\"  - {module_name}: {module_info['type']} with {module_info['parameter_count']:,} parameters\")\n",
    "    \n",
    "    return layers_info\n",
    "\n",
    "\n",
    "@app.function(image=image, gpu=\"any\", secrets=[huggingface_secret], timeout=600, volumes={MODEL_DIR: volume}, memory=16384)\n",
    "def get_embeddings(text, model_name):\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    import torch\n",
    "    \n",
    "    print(f\"Loading model and tokenizer: {model_name}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=MODEL_DIR, device_map=\"cuda\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=MODEL_DIR)\n",
    "    \n",
    "    # Get the embedding layer\n",
    "    embed_layer = model.model.embed_tokens\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Get embeddings for the tokens\n",
    "    with torch.no_grad():\n",
    "        embeddings = embed_layer(tokens.input_ids)\n",
    "    \n",
    "    # Return both the raw embeddings and some metadata\n",
    "    return {\n",
    "        \"embeddings\": embeddings.cpu().numpy(),\n",
    "        \"tokens\": tokens.input_ids.cpu().numpy(),\n",
    "        \"token_strings\": tokenizer.convert_ids_to_tokens(tokens.input_ids[0]),\n",
    "        \"embedding_dim\": embeddings.shape[-1]\n",
    "    }\n",
    "\n",
    "## Do inference on any text:\n",
    "@app.function(image=image, gpu=\"any\", secrets=[huggingface_secret], timeout=600, volumes={MODEL_DIR: volume}, memory=16384)\n",
    "def do_inference(text, model_name):\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    import torch\n",
    "    \n",
    "    print(f\"Loading model and tokenizer: {model_name}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=MODEL_DIR, device_map=\"cuda\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=MODEL_DIR)\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Get the embedding layer\n",
    "    out = model.generate(tokens.input_ids, max_new_tokens=1024)\n",
    "    return {'output': tokenizer.decode(out[0], skip_special_tokens=True)}\n",
    "    \n",
    "@app.function(image=image, gpu=\"any\", secrets=[huggingface_secret], timeout=600, volumes={MODEL_DIR: volume}, memory=16384)\n",
    "def get_intermediate_outputs(text, model_name, layer_num=4):\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    \n",
    "    print(f\"Loading model and tokenizer: {model_name}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=MODEL_DIR, device_map=\"cuda\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=MODEL_DIR)\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Run forward pass with output_hidden_states=True to get intermediate activations\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens.input_ids, output_hidden_states=True)\n",
    "    \n",
    "    # Get the hidden states (these are the intermediate layer outputs)\n",
    "    hidden_states = outputs.hidden_states\n",
    "    \n",
    "    # Print shapes of intermediate states for debugging\n",
    "    print(f\"Number of hidden states: {len(hidden_states)}\")\n",
    "    for i, hidden_state in enumerate(hidden_states):\n",
    "        print(f\"Layer {i} hidden state shape: {hidden_state.shape}\")\n",
    "    \n",
    "    # Extract the requested layer's output and convert to numpy for serialization\n",
    "    requested_layer_output = hidden_states[layer_num].cpu().numpy()\n",
    "    \n",
    "    # Return a serializable dictionary with the requested layer's output\n",
    "    return {\n",
    "        'layer_num': layer_num,\n",
    "        'layer_output_shape': requested_layer_output.shape,\n",
    "        'layer_output': requested_layer_output,\n",
    "        'token_strings': tokenizer.convert_ids_to_tokens(tokens.input_ids[0]),\n",
    "        'generated_text': tokenizer.decode(model.generate(tokens.input_ids, max_new_tokens=100)[0], skip_special_tokens=True)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA Model initialized\n",
      "\n",
      "Result summary:\n",
      "Model has 16 layers with hidden size 2048\n",
      "\n",
      "Layer details:\n",
      "  Layer 0: LlamaDecoderLayer with 60,821,504 parameters\n",
      "    - self_attention: LlamaAttention with 10,485,760 parameters\n",
      "    - mlp: LlamaMLP with 50,331,648 parameters\n",
      "    - input_layernorm: LlamaRMSNorm with 2,048 parameters\n",
      "    - post_attention_layernorm: LlamaRMSNorm with 2,048 parameters\n",
      "  Layer 1: LlamaDecoderLayer with 60,821,504 parameters\n",
      "    - self_attention: LlamaAttention with 10,485,760 parameters\n",
      "    - mlp: LlamaMLP with 50,331,648 parameters\n",
      "    - input_layernorm: LlamaRMSNorm with 2,048 parameters\n",
      "    - post_attention_layernorm: LlamaRMSNorm with 2,048 parameters\n",
      "  Layer 2: LlamaDecoderLayer with 60,821,504 parameters\n",
      "    - self_attention: LlamaAttention with 10,485,760 parameters\n",
      "    - mlp: LlamaMLP with 50,331,648 parameters\n",
      "    - input_layernorm: LlamaRMSNorm with 2,048 parameters\n",
      "    - post_attention_layernorm: LlamaRMSNorm with 2,048 parameters\n",
      "Ensuring app is shut down...\n"
     ]
    }
   ],
   "source": [
    "# For Jupyter Notebook execution\n",
    "try:\n",
    "    # Use a short app name to avoid potential path issues in Jupyter\n",
    "    with app.run():\n",
    "        print(\"LLaMA Model initialized\")\n",
    "        result = get_model_function.remote(model_name)\n",
    "        print(\"\\nResult summary:\")\n",
    "        print(f\"Model has {result['num_layers']} layers with hidden size {result['hidden_size']}\")\n",
    "        \n",
    "        # Display information about the first few layers\n",
    "        print(\"\\nLayer details:\")\n",
    "        for layer_info in result[\"all_layers\"][:3]:\n",
    "            print(f\"  Layer {layer_info['index']}: {layer_info['type']} with {layer_info['parameter_count']:,} parameters\")\n",
    "            if \"modules\" in layer_info:\n",
    "                for module_name, module_info in layer_info[\"modules\"].items():\n",
    "                    print(f\"    - {module_name}: {module_info['type']} with {module_info['parameter_count']:,} parameters\")\n",
    "finally:\n",
    "    # Force app to clean up in case of interrupts or cell re-execution\n",
    "    print(\"Ensuring app is shut down...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding Results:\n",
      "Input text: 'What sentence does the sequence of emoji represents:👩‍⚕️😷😢😄👥🏥🚑💔💉📋'\n",
      "Embedding dimension: 2048\n",
      "Number of tokens: 40\n",
      "\n",
      "Tokens:\n",
      "Token: <|begin_of_text|>\n",
      "Embedding shape: (2048,)\n",
      "First few values: [ 0.00268555  0.00308228 -0.00680542  0.04199219 -0.00265503]\n",
      "\n",
      "Token: What\n",
      "Embedding shape: (2048,)\n",
      "First few values: [ 0.02307129 -0.01190186 -0.00234985  0.02587891 -0.00546265]\n",
      "\n",
      "Token: Ġsentence\n",
      "Embedding shape: (2048,)\n",
      "First few values: [ 0.0135498  -0.01525879  0.03222656  0.00601196 -0.01696777]\n",
      "\n",
      "Token: Ġdoes\n",
      "Embedding shape: (2048,)\n",
      "First few values: [0.01965332 0.01507568 0.00350952 0.00927734 0.02246094]\n",
      "\n",
      "Token: Ġthe\n",
      "Embedding shape: (2048,)\n",
      "First few values: [-0.00805664 -0.01501465  0.02600098 -0.03808594 -0.00891113]\n",
      "\n",
      "Token: Ġsequence\n",
      "Embedding shape: (2048,)\n",
      "First few values: [-0.02563477  0.00588989 -0.0189209   0.08496094 -0.01672363]\n",
      "\n",
      "Token: Ġof\n",
      "Embedding shape: (2048,)\n",
      "First few values: [-0.00193787 -0.01403809  0.00527954 -0.02783203  0.00915527]\n",
      "\n",
      "Token: Ġemoji\n",
      "Embedding shape: (2048,)\n",
      "First few values: [-0.0014801  -0.00494385  0.01733398 -0.01208496 -0.00497437]\n",
      "\n",
      "Token: Ġrepresents\n",
      "Embedding shape: (2048,)\n",
      "First few values: [-0.02526855 -0.00454712 -0.00430298  0.04541016  0.00337219]\n",
      "\n",
      "Token: :\n",
      "Embedding shape: (2048,)\n",
      "First few values: [-0.02600098  0.00273132  0.04394531 -0.04443359  0.01141357]\n",
      "\n",
      "Token: ðŁ\n",
      "Embedding shape: (2048,)\n",
      "First few values: [ 0.02429199 -0.02172852  0.0100708   0.00421143 -0.02209473]\n",
      "\n",
      "Token: ĳ\n",
      "Embedding shape: (2048,)\n",
      "First few values: [-0.02148438  0.04101562  0.0300293  -0.00082779 -0.00283813]\n",
      "\n",
      "Token: ©\n",
      "Embedding shape: (2048,)\n",
      "First few values: [ 0.00787354  0.03955078  0.00671387 -0.0098877   0.02844238]\n",
      "\n",
      "Token: âĢį\n",
      "Embedding shape: (2048,)\n",
      "First few values: [ 0.00396729  0.01074219 -0.00436401 -0.00221252  0.01287842]\n",
      "\n",
      "Token: â\n",
      "Embedding shape: (2048,)\n",
      "First few values: [ 0.02038574 -0.00325012  0.02978516  0.05419922 -0.02429199]\n",
      "\n",
      "Token: ļ\n",
      "Embedding shape: (2048,)\n",
      "First few values: [ 0.00294495  0.03198242  0.01531982 -0.02648926 -0.04418945]\n",
      "\n",
      "Token: ķ\n",
      "Embedding shape: (2048,)\n",
      "First few values: [-0.02270508  0.00332642  0.02490234  0.03588867 -0.00094986]\n",
      "\n",
      "Token: ï¸ı\n",
      "Embedding shape: (2048,)\n",
      "First few values: [-0.00085449  0.03491211 -0.00521851  0.03955078  0.00083542]\n",
      "\n",
      "Token: ðŁĺ\n",
      "Embedding shape: (2048,)\n",
      "First few values: [ 0.02990723  0.01696777  0.00265503  0.01055908 -0.01434326]\n",
      "\n",
      "Token: ·\n",
      "Embedding shape: (2048,)\n",
      "First few values: [ 0.03027344  0.0168457  -0.02160645  0.01123047 -0.00257874]\n",
      "\n",
      "Token: ðŁĺ\n",
      "Embedding shape: (2048,)\n",
      "First few values: [ 0.02990723  0.01696777  0.00265503  0.01055908 -0.01434326]\n",
      "\n",
      "Token: ¢\n",
      "Embedding shape: (2048,)\n",
      "First few values: [ 0.02441406  0.0039978   0.02563477  0.01477051 -0.00952148]\n",
      "\n",
      "Token: ðŁĺ\n",
      "Embedding shape: (2048,)\n",
      "First few values: [ 0.02990723  0.01696777  0.00265503  0.01055908 -0.01434326]\n",
      "\n",
      "Token: Ħ\n",
      "Embedding shape: (2048,)\n",
      "First few values: [-0.0039978  -0.03759766 -0.0534668  -0.02124023 -0.02062988]\n",
      "\n",
      "Token: ðŁ\n",
      "Embedding shape: (2048,)\n",
      "First few values: [ 0.02429199 -0.02172852  0.0100708   0.00421143 -0.02209473]\n",
      "\n",
      "Token: ĳ\n",
      "Embedding shape: (2048,)\n",
      "First few values: [-0.02148438  0.04101562  0.0300293  -0.00082779 -0.00283813]\n",
      "\n",
      "Token: ¥\n",
      "Embedding shape: (2048,)\n",
      "First few values: [ 0.00283813  0.05639648 -0.00631714  0.02001953 -0.0078125 ]\n",
      "\n",
      "Token: ðŁ\n",
      "Embedding shape: (2048,)\n",
      "First few values: [ 0.02429199 -0.02172852  0.0100708   0.00421143 -0.02209473]\n",
      "\n",
      "Token: ı\n",
      "Embedding shape: (2048,)\n",
      "First few values: [ 0.0402832   0.01477051  0.01733398 -0.01220703 -0.00982666]\n",
      "\n",
      "Token: ¥\n",
      "Embedding shape: (2048,)\n",
      "First few values: [ 0.00283813  0.05639648 -0.00631714  0.02001953 -0.0078125 ]\n",
      "\n",
      "Token: ðŁ\n",
      "Embedding shape: (2048,)\n",
      "First few values: [ 0.02429199 -0.02172852  0.0100708   0.00421143 -0.02209473]\n",
      "\n",
      "Token: ļ\n",
      "Embedding shape: (2048,)\n",
      "First few values: [ 0.00294495  0.03198242  0.01531982 -0.02648926 -0.04418945]\n",
      "\n",
      "Token: ĳ\n",
      "Embedding shape: (2048,)\n",
      "First few values: [-0.02148438  0.04101562  0.0300293  -0.00082779 -0.00283813]\n",
      "\n",
      "Token: ðŁĴ\n",
      "Embedding shape: (2048,)\n",
      "First few values: [ 0.02233887  0.01098633  0.00273132 -0.01696777 -0.03369141]\n",
      "\n",
      "Token: Ķ\n",
      "Embedding shape: (2048,)\n",
      "First few values: [-0.00405884  0.01574707 -0.0279541   0.02355957  0.02734375]\n",
      "\n",
      "Token: ðŁĴ\n",
      "Embedding shape: (2048,)\n",
      "First few values: [ 0.02233887  0.01098633  0.00273132 -0.01696777 -0.03369141]\n",
      "\n",
      "Token: ī\n",
      "Embedding shape: (2048,)\n",
      "First few values: [-0.0098877   0.04199219  0.01831055 -0.0291748  -0.01733398]\n",
      "\n",
      "Token: ðŁ\n",
      "Embedding shape: (2048,)\n",
      "First few values: [ 0.02429199 -0.02172852  0.0100708   0.00421143 -0.02209473]\n",
      "\n",
      "Token: ĵ\n",
      "Embedding shape: (2048,)\n",
      "First few values: [-0.00482178  0.00140381  0.01202393  0.00836182  0.02294922]\n",
      "\n",
      "Token: ĭ\n",
      "Embedding shape: (2048,)\n",
      "First few values: [ 0.02294922  0.01733398 -0.01373291 -0.01782227 -0.00976562]\n",
      "\n",
      "Inference result: What sentence does the sequence of emoji represents:👩‍⚕️😷😢😄👥🏥🚑💔💉📋\n",
      "\n",
      "The sequence of emojis represents a patient's journey through a hospital. Here's a breakdown of each emoji:\n",
      "\n",
      "1. 👩‍⚕️ - A female doctor (representing the medical staff)\n",
      "2. 😷 - A face with tears (representing the patient's emotional state)\n",
      "3. 😢 - A crying face (representing the patient's sadness)\n",
      "4. 😄 - A smiling face (representing the patient's recovery)\n",
      "5. 👥 - A group of people (representing the hospital staff and the patient's support system)\n",
      "6. 🏥 - A hospital (representing the physical environment)\n",
      "7. 🚑 - A ambulance (representing the emergency services)\n",
      "8. 💔 - A broken heart (representing the emotional impact of the illness)\n",
      "9. 💉 - A syringe (representing the medical treatment)\n",
      "10. 📋 - A doctor's notebook (representing the patient's recovery process)\n",
      "\n",
      "The sequence of emojis represents a patient's journey through the hospital, from initial illness to recovery and back to normal life. The doctor and hospital staff are represented by the first two emojis, the patient's emotional state is represented by the next three emojis, and the recovery process is represented by the next four emojis. The emotional impact of the illness is represented by the broken heart emoji, and the medical treatment is represented by the syringe emoji. The recovery process is represented by the doctor's notebook emoji.\n",
      "App is closing\n"
     ]
    }
   ],
   "source": [
    "# For Jupyter Notebook execution\n",
    "# Define the variable outside the context manager scope\n",
    "embedding_result = None\n",
    "inference_result = None\n",
    "try:\n",
    "    with app.run():\n",
    "        # Test with a sample text\n",
    "        sample_text = \"What sentence does the sequence of emoji represents:👩‍⚕️😷😢😄👥🏥🚑💔💉📋\"\n",
    "        result = get_embeddings.remote(sample_text, model_name)\n",
    "        \n",
    "        # Store the result in the outer variable\n",
    "        embedding_result = result\n",
    "        \n",
    "        print(\"\\nEmbedding Results:\")\n",
    "        print(f\"Input text: '{sample_text}'\")\n",
    "        print(f\"Embedding dimension: {result['embedding_dim']}\")\n",
    "        print(f\"Number of tokens: {len(result['token_strings'])}\")\n",
    "        print(\"\\nTokens:\")\n",
    "        for token, embedding in zip(result['token_strings'], result['embeddings'][0]):\n",
    "            print(f\"Token: {token}\")\n",
    "            print(f\"Embedding shape: {embedding.shape}\")\n",
    "            print(f\"First few values: {embedding[:5]}\\n\")\n",
    "        inference_result = do_inference.remote(sample_text, model_name)\n",
    "        print(f\"Inference result: {inference_result['output']}\")\n",
    "finally:\n",
    "    print(\"App is closing\")\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|>', 'What', 'Ġdoes', 'Ġthis', 'Ġemoji', 'Ġmean', '?', 'ðŁ', 'ĳ', '©', 'âĢį', 'â', 'ļ', 'ķ', 'ï¸ı', 'ðŁĺ', '·', 'ðŁĺ', '¢', 'ðŁĺ', 'Ħ', 'ðŁ', 'ĳ', '¥', 'ðŁ', 'ı', '¥', 'ðŁ', 'ļ', 'ĳ', 'ðŁĴ', 'Ķ', 'ðŁĴ', 'ī', 'ðŁ', 'ĵ', 'ĭ']\n"
     ]
    }
   ],
   "source": [
    "print(embedding_result['token_strings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What sentence does the sequence of emoji represents:👩‍⚕️😷😢😄👥🏥🚑💔💉📋\n",
      "\n",
      "The sequence of emojis represents a patient's journey through a hospital. Here's a breakdown of each emoji:\n",
      "\n",
      "1. 👩‍⚕️ - A female doctor (representing the medical staff)\n",
      "2. 😷 - A face with tears (representing the patient's emotional state)\n",
      "3. 😢 - A crying face (representing the patient's sadness)\n",
      "4. 😄 - A smiling face (representing the patient's recovery)\n",
      "5. 👥 - A group of people (representing the hospital staff and the patient's support system)\n",
      "6. 🏥 - A hospital (representing the physical environment)\n",
      "7. 🚑 - A ambulance (representing the emergency services)\n",
      "8. 💔 - A broken heart (representing the emotional impact of the illness)\n",
      "9. 💉 - A syringe (representing the medical treatment)\n",
      "10. 📋 - A doctor's notebook (representing the patient's recovery process)\n",
      "\n",
      "The sequence of emojis represents a patient's journey through the hospital, from initial illness to recovery and back to normal life. The doctor and hospital staff are represented by the first two emojis, the patient's emotional state is represented by the next three emojis, and the recovery process is represented by the next four emojis. The emotional impact of the illness is represented by the broken heart emoji, and the medical treatment is represented by the syringe emoji. The recovery process is represented by the doctor's notebook emoji.\n"
     ]
    }
   ],
   "source": [
    "print(inference_result['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Intermediate Layer Results:\n",
      "Input text: 'Hello, world!'\n",
      "Layer number: 4\n",
      "Layer output shape: (1, 5, 2048)\n",
      "Number of tokens: 5\n",
      "\n",
      "Tokens:\n",
      "\n",
      "Token: <|begin_of_text|>\n",
      "Vector shape: (2048,)\n",
      "First few values: [ 0.12739815 -0.501549    1.5829394  -0.34711745  0.48629034]\n",
      "\n",
      "Token: Hello\n",
      "Vector shape: (2048,)\n",
      "First few values: [0.05333915 0.01220436 0.02671797 0.01382664 0.16754065]\n",
      "\n",
      "Token: ,\n",
      "Vector shape: (2048,)\n",
      "First few values: [ 0.07111846 -0.01006559  0.1203342   0.04470684  0.10219292]\n",
      "\n",
      "Token: Ġworld\n",
      "Vector shape: (2048,)\n",
      "First few values: [ 0.01662472 -0.02124677 -0.03644049 -0.02957273  0.06016726]\n",
      "\n",
      "Token: !\n",
      "Vector shape: (2048,)\n",
      "First few values: [-0.04820976 -0.03211864 -0.02830466  0.08335431 -0.02354628]\n",
      "\n",
      "Generated text: Hello, world! I'm excited to be here. My name is Max, and I'm a software engineer with a passion for building scalable and efficient systems.\n",
      "\n",
      "As a seasoned developer, I've had the privilege of working on various projects, from mobile apps to web applications, and I'm always looking for new challenges and opportunities to improve my skills.\n",
      "\n",
      "I'm particularly interested in cloud computing, artificial intelligence, and machine learning, and I've been experimenting with these technologies in my free time. I'm also a big fan\n",
      "Ensuring app is shut down...\n"
     ]
    }
   ],
   "source": [
    "# For Jupyter Notebook execution\n",
    "intermediate_result = None\n",
    "try:\n",
    "    with app.run():\n",
    "        # Test with a sample text\n",
    "        sample_text = \"Hello, world!\"\n",
    "        layer_num = 4  # Get outputs after the 4th layer\n",
    "        result = get_intermediate_outputs.remote(sample_text, model_name, layer_num)\n",
    "        intermediate_result = result\n",
    "        print(\"\\nIntermediate Layer Results:\")\n",
    "        print(f\"Input text: '{sample_text}'\")\n",
    "        print(f\"Layer number: {result['layer_num']}\")\n",
    "        print(f\"Layer output shape: {result['layer_output_shape']}\")\n",
    "        print(f\"Number of tokens: {len(result['token_strings'])}\")\n",
    "        print(\"\\nTokens:\")\n",
    "        for i, token in enumerate(result['token_strings']):\n",
    "            print(f\"\\nToken: {token}\")\n",
    "            token_vector = result['layer_output'][0, i]\n",
    "            print(f\"Vector shape: {token_vector.shape}\")\n",
    "            print(f\"First few values: {token_vector[:5]}\")\n",
    "        \n",
    "        print(f\"\\nGenerated text: {result['generated_text']}\")\n",
    "            \n",
    "finally:\n",
    "    print(\"Ensuring app is shut down...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5, 2048)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_result[\"layer_output\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
