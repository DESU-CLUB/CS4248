{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Simple Testing of LLaMA encoder\n",
    "- We will be using the following tech stack\n",
    "\n",
    "   - HuggingFace Transformers\n",
    " \n",
    "   - Modal GPU credits for local file gpu usage\n",
    " \n",
    "  - EmojiLM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef5d540d6b254762b42230bf5885e557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import huggingface_hub\n",
    "import os\n",
    "load_dotenv()\n",
    "huggingface_hub.notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14289/467177460.py:24: DeprecationError: 2025-02-03: Modal will stop implicitly adding local Python modules to the Image (\"automounting\") in a future update. The following modules need to be explicitly added for future compatibility:\n",
      "* _remote_module_non_scriptable\n",
      "\n",
      "e.g.:\n",
      "image_with_source = my_image.add_local_python_source(\"_remote_module_non_scriptable\")\n",
      "\n",
      "For more information, see https://modal.com/docs/guide/modal-1-0-migration\n",
      "  def get_model_function(model_name):\n"
     ]
    }
   ],
   "source": [
    "# Firstly, grab the LLaMA encoder and the dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import modal\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "huggingface_secret = modal.Secret.from_name(\n",
    "    \"huggingface-secret\", required_keys=[\"HF_TOKEN\"]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "volume = modal.Volume.from_name(\n",
    "    \"llama_embeddings\", create_if_missing=True\n",
    ")\n",
    "MODEL_DIR = \"/model\"\n",
    "\n",
    "# Define stubs at module level, outside the class\n",
    "app = modal.App(name=\"llama-embeddings\")\n",
    "image = modal.Image.debian_slim(python_version = '3.10').pip_install([\"transformers\", \"torch\", \"accelerate\", \"hf_transfer\"])\n",
    "image = image.env(\n",
    "    {\"HF_HUB_ENABLE_HF_TRANSFER\": \"1\"}  # turn on faster downloads from HF\n",
    ")\n",
    "\n",
    "@app.function(image=image, gpu=\"any\", secrets=[huggingface_secret], timeout=600, volumes={MODEL_DIR: volume}, memory=16384)\n",
    "def get_model_function(model_name):\n",
    "    from transformers import AutoModelForCausalLM\n",
    "    import os\n",
    "    \n",
    "    print(f\"Starting to load model: {model_name}\")\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    \n",
    "    # Log the model being loaded\n",
    "    print(\"Loading model... (this may take a few minutes)\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=MODEL_DIR, device_map=\"cuda\")\n",
    "    print(f\"Model loaded successfully!\")\n",
    "    \n",
    "    # Print model architecture details\n",
    "    print(f\"Model architecture: {model.__class__.__name__}\")\n",
    "    print(f\"Number of layers: {len(model.model.layers)}\")\n",
    "    \n",
    "    # Create serializable information about some sample layers\n",
    "    layers_info = {\n",
    "        \"num_layers\": len(model.model.layers),\n",
    "        \"hidden_size\": model.config.hidden_size if hasattr(model.config, \"hidden_size\") else \"Unknown\"\n",
    "    }\n",
    "    \n",
    "    # Information for all layers to make it serializable\n",
    "    layers_info[\"all_layers\"] = []\n",
    "    for i in range(len(model.model.layers)):\n",
    "        layer = model.model.layers[i]\n",
    "        layer_info = {\n",
    "            \"index\": i,\n",
    "            \"type\": layer.__class__.__name__,\n",
    "            \"parameter_count\": sum(p.numel() for p in layer.parameters())\n",
    "        }\n",
    "        \n",
    "        # Extract information about modules in the decoder layer\n",
    "        if hasattr(layer, \"self_attn\"):\n",
    "            layer_info[\"modules\"] = {\n",
    "                \"self_attention\": {\n",
    "                    \"type\": layer.self_attn.__class__.__name__,\n",
    "                    \"parameter_count\": sum(p.numel() for p in layer.self_attn.parameters())\n",
    "                },\n",
    "                \"mlp\": {\n",
    "                    \"type\": layer.mlp.__class__.__name__,\n",
    "                    \"parameter_count\": sum(p.numel() for p in layer.mlp.parameters())\n",
    "                }\n",
    "            }\n",
    "            if hasattr(layer, \"input_layernorm\"):\n",
    "                layer_info[\"modules\"][\"input_layernorm\"] = {\n",
    "                    \"type\": layer.input_layernorm.__class__.__name__,\n",
    "                    \"parameter_count\": sum(p.numel() for p in layer.input_layernorm.parameters())\n",
    "                }\n",
    "            if hasattr(layer, \"post_attention_layernorm\"):\n",
    "                layer_info[\"modules\"][\"post_attention_layernorm\"] = {\n",
    "                    \"type\": layer.post_attention_layernorm.__class__.__name__,\n",
    "                    \"parameter_count\": sum(p.numel() for p in layer.post_attention_layernorm.parameters())\n",
    "                }\n",
    "        \n",
    "        layers_info[\"all_layers\"].append(layer_info)\n",
    "        \n",
    "        # Print details for the first few layers\n",
    "        if i < 3:\n",
    "            print(f\"Layer {i}: {layer_info['type']} with {layer_info['parameter_count']:,} parameters\")\n",
    "            if \"modules\" in layer_info:\n",
    "                for module_name, module_info in layer_info[\"modules\"].items():\n",
    "                    print(f\"  - {module_name}: {module_info['type']} with {module_info['parameter_count']:,} parameters\")\n",
    "    \n",
    "    return layers_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA Model initialized\n",
      "\n",
      "Result summary:\n",
      "Model has 16 layers with hidden size 2048\n",
      "\n",
      "Layer details:\n",
      "  Layer 0: LlamaDecoderLayer with 60,821,504 parameters\n",
      "    - self_attention: LlamaAttention with 10,485,760 parameters\n",
      "    - mlp: LlamaMLP with 50,331,648 parameters\n",
      "    - input_layernorm: LlamaRMSNorm with 2,048 parameters\n",
      "    - post_attention_layernorm: LlamaRMSNorm with 2,048 parameters\n",
      "  Layer 1: LlamaDecoderLayer with 60,821,504 parameters\n",
      "    - self_attention: LlamaAttention with 10,485,760 parameters\n",
      "    - mlp: LlamaMLP with 50,331,648 parameters\n",
      "    - input_layernorm: LlamaRMSNorm with 2,048 parameters\n",
      "    - post_attention_layernorm: LlamaRMSNorm with 2,048 parameters\n",
      "  Layer 2: LlamaDecoderLayer with 60,821,504 parameters\n",
      "    - self_attention: LlamaAttention with 10,485,760 parameters\n",
      "    - mlp: LlamaMLP with 50,331,648 parameters\n",
      "    - input_layernorm: LlamaRMSNorm with 2,048 parameters\n",
      "    - post_attention_layernorm: LlamaRMSNorm with 2,048 parameters\n",
      "Ensuring app is shut down...\n"
     ]
    }
   ],
   "source": [
    "# For Jupyter Notebook execution\n",
    "try:\n",
    "    # Use a short app name to avoid potential path issues in Jupyter\n",
    "    with app.run():\n",
    "        print(\"LLaMA Model initialized\")\n",
    "        result = get_model_function.remote(model_name)\n",
    "        print(\"\\nResult summary:\")\n",
    "        print(f\"Model has {result['num_layers']} layers with hidden size {result['hidden_size']}\")\n",
    "        \n",
    "        # Display information about the first few layers\n",
    "        print(\"\\nLayer details:\")\n",
    "        for layer_info in result[\"all_layers\"][:3]:\n",
    "            print(f\"  Layer {layer_info['index']}: {layer_info['type']} with {layer_info['parameter_count']:,} parameters\")\n",
    "            if \"modules\" in layer_info:\n",
    "                for module_name, module_info in layer_info[\"modules\"].items():\n",
    "                    print(f\"    - {module_name}: {module_info['type']} with {module_info['parameter_count']:,} parameters\")\n",
    "finally:\n",
    "    # Force app to clean up in case of interrupts or cell re-execution\n",
    "    print(\"Ensuring app is shut down...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
