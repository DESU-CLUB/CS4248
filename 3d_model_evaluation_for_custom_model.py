# -*- coding: utf-8 -*-
"""3D Model Evaluation for Custom Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15fyom60nMgFIAZhCfXSxZJthnRJjTZ2w
"""

from google.colab import files
files.upload()

"""Embedding 3D"""

!pip install transformers scikit-learn matplotli

!pip install -q datasets

# Step 1: Load dataset
from datasets import load_dataset
ds = load_dataset("DESUCLUB/combined_emoji_data")
df = ds["train"].to_pandas()

# # Step 2: Sample N rows (e.g., 300 samples)
# df_sample = df[df["text"].notnull()].sample(n=300, random_state=42)
# texts = df_sample["text"].tolist()

import os
os.environ["OPENAI_API_KEY"]="APIKEY"

import openai
client = openai.OpenAI()

response = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": f"Here are 100 sample user messages:\n{joined_text}\n\nPlease identify 10 to 15 common topics these texts fall under. Just list the topics as a numbered list."}
    ],
    temperature=0.7
)

print(response.choices[0].message.content)

df_sample = df[df["text"].notnull()].sample(n=500, random_state=42).reset_index(drop=True)
texts = df_sample["text"].tolist()

from transformers import BertTokenizer, BertModel

bert_tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
bert_model = BertModel.from_pretrained("bert-base-uncased").to(device)
bert_model.eval()

# Get BERT embeddings
import torch

def get_embeddings(text_list, tokenizer, model, is_custom=False):
    all_embeddings = []
    batch_size = 32
    for i in range(0, len(text_list), batch_size):
        batch = text_list[i:i+batch_size]
        tokens = tokenizer(batch, return_tensors="pt", padding=True, truncation=True).to(device)

        with torch.no_grad():
            if is_custom:
                # Custom model: expects only input_ids
                out = model(tokens["input_ids"])  # e.g. (B, 2048)
            else:
                # HuggingFace model: standard forward
                out = model(**tokens).last_hidden_state.mean(dim=1)

            all_embeddings.append(out.cpu())

    return torch.cat(all_embeddings)

bert_embeddings = get_embeddings(texts, bert_tokenizer, bert_model, is_custom=False)

your_embeddings = get_embeddings(texts, tokenizer, encoder, is_custom=True)

import torch.nn as nn

# Only define once, outside loops
projection = nn.Linear(2048, 768).to(device)

# Apply projection to match BERT embedding dimension
projected_embeddings = projection(your_embeddings.to(device))

topic_names = [
    "Outdoor Activities and Nature", "Personal Emotions and Feelings", "Travel and Tourism",
    "Arts and Creativity", "Sports and Fitness", "Food and Drink", "Home and Lifestyle",
    "Tools and DIY Projects", "Career and Education", "Vehicles and Transportation",
    "Animals and Wildlife", "Weather and Seasons", "Health and Well-being",
    "Personal Relationships and Social Life", "Architecture and Design"
]

# Option A: If using OpenAI (youâ€™ll need API key)
# Option B: Use the same BERT model to get topic embeddings:
topic_embeddings = get_embeddings(topic_names, bert_tokenizer, bert_model)

import torch.nn.functional as F

def assign_topics(text_embeddings, topic_embeddings, topic_labels):
    # Move topic_embeddings to same device as text
    topic_embeddings = topic_embeddings.to(text_embeddings.device)

    similarities = F.cosine_similarity(
        text_embeddings.unsqueeze(1), topic_embeddings.unsqueeze(0), dim=-1
    )  # (num_texts, num_topics)

    top_indices = similarities.argmax(dim=1)
    assigned = [topic_labels[i] for i in top_indices]
    return assigned


assigned_topics_bert = assign_topics(bert_embeddings, topic_embeddings, topic_names)
assigned_topics_custom = assign_topics(projected_embeddings, topic_embeddings, topic_names)

from sklearn.manifold import TSNE
import pandas as pd
import plotly.express as px

def plot_embeddings_3d(embeddings, labels, title):
    tsne = TSNE(n_components=3, random_state=42)
    reduced = tsne.fit_transform(embeddings.numpy())
    df_plot = pd.DataFrame(reduced, columns=["x", "y", "z"])
    df_plot["topic"] = labels

    fig = px.scatter_3d(df_plot, x="x", y="y", z="z", color="topic", title=title)
    fig.show()

plot_embeddings_3d(bert_embeddings, assigned_topics_bert, "BERT Embedding Clusters")
plot_embeddings_3d(your_embeddings, assigned_topics_custom, "Custom Encoder Clusters")

import pandas as pd

# Convert topic labels (strings) to integer codes
topic_to_id = {topic: i for i, topic in enumerate(sorted(set(assigned_topics_bert + assigned_topics_custom)))}
topic_ids_bert = [topic_to_id[t] for t in assigned_topics_bert]
topic_ids_custom = [topic_to_id[t] for t in assigned_topics_custom]

# BERT plot
fig.add_trace(
    go.Scatter3d(
        x=bert_embeddings_3d[:, 0],
        y=bert_embeddings_3d[:, 1],
        z=bert_embeddings_3d[:, 2],
        mode='markers',
        marker=dict(size=4, color=topic_ids_bert, colorscale='Viridis'),
        text=assigned_topics_bert,
        name="BERT"
    ),
    row=1, col=1
)

# Custom Encoder plot
fig.add_trace(
    go.Scatter3d(
        x=custom_embeddings_3d[:, 0],
        y=custom_embeddings_3d[:, 1],
        z=custom_embeddings_3d[:, 2],
        mode='markers',
        marker=dict(size=4, color=topic_ids_custom, colorscale='Viridis'),
        text=assigned_topics_custom,
        name="Custom"
    ),
    row=1, col=2
)

import torch
import pandas as pd

# Store embeddings + assigned topics
df_embed = pd.DataFrame(projected_embeddings.detach().cpu().numpy())
df_embed["topic"] = assigned_topics_custom


# Average embedding per predicted cluster
cluster_avg = torch.tensor(
    df_embed.groupby("topic")
            .mean()
            .values
).to(torch.float32)  # shape: (num_clusters, 768)

import torch.nn.functional as F

# Make sure both tensors are on same device
topic_embeddings_tensor = topic_embeddings.to(cluster_avg.device)

# (num_clusters, num_topics)
similarity_matrix = F.cosine_similarity(
    cluster_avg.unsqueeze(1),           # (C, 1, D)
    topic_embeddings_tensor.unsqueeze(0), # (1, T, D)
    dim=-1
)

import seaborn as sns
import matplotlib.pyplot as plt

# Convert to pandas DataFrame
heatmap_df = pd.DataFrame(similarity_matrix.cpu().numpy(),
                          index=sorted(set(assigned_topics_custom)),  # clusters
                          columns=topic_names)                         # GPT topics

plt.figure(figsize=(12, 6))
sns.heatmap(heatmap_df, annot=True, fmt=".2f", cmap="viridis")
plt.title("Cosine Similarity Between Cluster Avg Embeddings and GPT Topics")
plt.ylabel("Assigned Topic Cluster")
plt.xlabel("GPT Topic Embedding")
plt.tight_layout()
plt.show()

import torch
import torch.nn.functional as F

# Assume these are defined:
# - bert_embeddings: shape [N, 768], from BERT
# - projected_embeddings: shape [N, 768], from your encoder
# - assigned_topics_bert: list of cluster names or topic indices
# - assigned_topics_custom: list of cluster names or topic indices
# - topic_embeddings: shape [15, 768], GPT topic embeddings
# - topic_labels: list of 15 topic names

def compute_avg_cluster_embeddings(embeddings, assigned_topics):
    df = pd.DataFrame(embeddings.detach().cpu().numpy())
    df["assigned_topic"] = assigned_topics
    avg_embeddings = []
    unique_topics = sorted(df["assigned_topic"].unique())
    for topic in unique_topics:
        topic_mean = df[df["assigned_topic"] == topic].iloc[:, :-1].mean(axis=0)
        avg_embeddings.append(topic_mean.values)
    return torch.tensor(avg_embeddings), unique_topics

def compute_topic_similarity(cluster_embeddings, topic_embeddings):
    cluster_embeddings = cluster_embeddings.to(topic_embeddings.device)
    similarities = F.cosine_similarity(
        cluster_embeddings.unsqueeze(1),  # (clusters, 1, D)
        topic_embeddings.unsqueeze(0),    # (1, topics, D)
        dim=-1                            # (clusters, topics)
    )
    return similarities.cpu().numpy()

# 1. Cluster-wise averages
bert_cluster_avg, assigned_topic_names_bert = compute_avg_cluster_embeddings(bert_embeddings, assigned_topics_bert)
custom_cluster_avg, assigned_topic_names_custom = compute_avg_cluster_embeddings(projected_embeddings, assigned_topics_custom)

# 2. Similarity matrix
bert_topic_sim = compute_topic_similarity(bert_cluster_avg, topic_embeddings)
custom_topic_sim = compute_topic_similarity(custom_cluster_avg, topic_embeddings)

topic_labels = [
    "Outdoor Activities and Nature",
    "Personal Emotions and Feelings",
    "Travel and Tourism",
    "Arts and Creativity",
    "Sports and Fitness",
    "Food and Drink",
    "Home and Lifestyle",
    "Tools and DIY Projects",
    "Career and Education",
    "Vehicles and Transportation",
    "Animals and Wildlife",
    "Weather and Seasons",
    "Health and Well-being",
    "Personal Relationships and Social Life",
    "Architecture and Design"
]

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Optional: convert to DataFrames for axis labeling
import pandas as pd

bert_df = pd.DataFrame(bert_topic_sim, columns=topic_labels)
bert_df['Assigned Cluster'] = assigned_topic_names_bert  # optional for row labeling

custom_df = pd.DataFrame(custom_topic_sim, columns=topic_labels)
custom_df['Assigned Cluster'] = assigned_topic_names_custom  # optional

# Plot
fig, axes = plt.subplots(1, 2, figsize=(22, 8), sharey=True)

sns.heatmap(
    bert_df.drop(columns='Assigned Cluster').values,
    annot=True, fmt=".2f", cmap="viridis", ax=axes[0],
    yticklabels=bert_df['Assigned Cluster']
)
axes[0].set_title("ðŸ”µ BERT: Cosine Similarity with GPT Topics")
axes[0].set_xlabel("GPT Topic Embedding")
axes[0].set_ylabel("Assigned BERT Topic Cluster")

sns.heatmap(
    custom_df.drop(columns='Assigned Cluster').values,
    annot=True, fmt=".2f", cmap="viridis", ax=axes[1],
    yticklabels=custom_df['Assigned Cluster']
)
axes[1].set_title("ðŸŸ£ Custom Encoder: Cosine Similarity with GPT Topics")
axes[1].set_xlabel("GPT Topic Embedding")
axes[1].set_ylabel("")  # no label, shared with left plot

plt.tight_layout()
plt.show()